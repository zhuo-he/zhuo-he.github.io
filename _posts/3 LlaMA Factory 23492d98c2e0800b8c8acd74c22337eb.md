# 3. LlaMA Factory

# 1. ä»å‘½ä»¤è¡Œå®šä½åˆ°ç¨‹åºå…¥å£

æˆ‘ä»¬é¦–å…ˆä» example æ–‡ä»¶ä¸­æ‰¾åˆ°å¯åŠ¨å‘½ä»¤ï¼š**examples/README_zh.md**

```python
llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
```

å¦‚æœä¹‹å‰æ²¡æ¥è§¦è¿‡ï¼Œé‚£ä¹ˆé¦–å…ˆé‡åˆ°çš„é—®é¢˜æ˜¯ `llamafactory-cli` æ˜¯ä»€ä¹ˆï¼Ÿ

`llamafactory-cli` æ˜¯ **LLaMA-Factory** æä¾›çš„ä¸€ä¸ªå‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºç»Ÿä¸€æ‰§è¡Œè®­ç»ƒã€æ¨ç†ã€è¯„ä¼°ã€å¯¼å‡ºç­‰ä»»åŠ¡ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹`llamafactory-cli` æ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼š

é¦–å…ˆï¼Œå®˜æ–¹ README æ–‡æ¡£ä¸­ Getting Start è¡¨æ˜ï¼ŒInstall from Source éœ€è¦æœ‰ä¸€è¡Œå‘½ä»¤

```python
pip install -e ".[torch,metrics]" --no-build-isolation
```

å…¶ä¸­ï¼Œ

`pip install -e` è¡¨ç¤º editable installï¼Œè¡¨ç¤ºå¯ç¼–è¾‘å®‰è£…ï¼Œå®ƒä¸ä¼šæŠŠä½ çš„æºç æ‹·è´åˆ° site-packagesï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªæŒ‡å‘æºç ç›®å½•çš„è½¯é“¾æ¥ã€‚ä¼˜ç‚¹æ˜¯**ä¿®æ”¹æºç åï¼Œç«‹åˆ»ç”Ÿæ•ˆ**ï¼Œä¸ç”¨é‡æ–°å®‰è£…ï¼Œç‰¹åˆ«é€‚åˆå¼€å‘è°ƒè¯•ã€‚`.[torch,metrics]` ä¸­

`.` è¡¨ç¤ºå½“å‰ç›®å½•æ˜¯ä¸€ä¸ª python é¡¹ç›®ï¼Œæœ‰ setup.py æˆ– pyproject.tomlï¼Œå¦‚æœæ£€æµ‹åˆ°ä½ é¡¹ç›®é‡Œæœ‰ pyproject.toml å¹¶å£°æ˜äº† [build-systerm]ï¼Œpip ä¼šé»˜è®¤å¯ç”¨ build isolationã€‚åœ¨ LlaMA Factory ä¸­ï¼Œæˆ‘ä»¬çœ‹ **pyproject.toml** ä¸­å­˜åœ¨ï¼š

```python
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"
```

é‚£ä¹ˆæ­¤æ—¶ä¼šåˆ†åˆ«æ‰§è¡Œï¼š

- åˆ›å»ºä¸´æ—¶ç¯å¢ƒ
- å®‰è£… setuptoolsâ‰¥61.0
- ä½¿ç”¨ setuptools.build_meta æ„å»º wheelï¼Œå³é™æ€è¯»å– setup.py ä¸­è°ƒç”¨çš„å‚æ•°ï¼ˆä¸è¿è¡Œ main å‡½æ•°ï¼‰ï¼Œéšåæ„å»º wheel åŒ…

æˆ‘ä»¬ç»§ç»­çœ‹ **setup.py** ä¸­çš„ main å‡½æ•°ï¼š

```python
def main():
    setup(
        name="llamafactory",
        version=get_version(),
        author="hiyouga",
        author_email="hiyouga@buaa.edu.cn",
        description="Unified Efficient Fine-Tuning of 100+ LLMs",
        long_description=open("README.md", encoding="utf-8").read(),
        long_description_content_type="text/markdown",
        keywords=["AI", "LLM", "GPT", "ChatGPT", "Llama", "Transformer", "DeepSeek", "Pytorch"],
        license="Apache 2.0 License",
        url="https://github.com/hiyouga/LLaMA-Factory",
        package_dir={"": "src"},
        packages=find_packages("src"),
        python_requires=">=3.9.0",
        install_requires=get_requires(),
        extras_require=extra_require,
        entry_points={"console_scripts": get_console_scripts()},
        classifiers=[
            "Development Status :: 4 - Beta",
            "Intended Audience :: Developers",
            "Intended Audience :: Education",
            "Intended Audience :: Science/Research",
            "License :: OSI Approved :: Apache Software License",
            "Operating System :: OS Independent",
            "Programming Language :: Python :: 3",
            "Programming Language :: Python :: 3.9",
            "Programming Language :: Python :: 3.10",
            "Programming Language :: Python :: 3.11",
            "Programming Language :: Python :: 3.12",
            "Topic :: Scientific/Engineering :: Artificial Intelligence",
        ],
    )
```

æˆ‘ä»¬ä¸€æ­¥æ­¥æ‹†è§£ï¼Œçœ‹å®ƒæ˜¯æ€ä¹ˆæ„å»º wheel çš„ã€‚

- `name="llamafactory"` + `version=get_version()` ï¼šå†³å®šäº† .whl çš„æ–‡ä»¶å
- `packages=find_packages("src")` + `package_dir={"": "src"}`ï¼š`find_packages("src")` ä¼šåœ¨ src/ ä¸‹é€’å½’æŸ¥æ‰¾æ‰€æœ‰å«æœ‰ __init__.py çš„ç›®å½•ï¼ˆä¹Ÿå°±æ˜¯ Python åŒ…ï¼‰ï¼Œ`package_dir={"": "src"}` è¡¨ç¤ºå°†é€»è¾‘ä¸Šçš„é¡¶çº§æ¨¡å—è·¯å¾„ â€œâ€ æ˜ å°„åˆ° src/ ï¼Œå°±æ˜¯è¯´ï¼šllamafactory è¿™ä¸ªé¡¶å±‚æ¨¡å—ï¼Œå…¶å®ç‰©ç†è·¯å¾„æ˜¯ src/llamafactoryã€‚æ€»çš„æ¥è¯´ï¼Œè¿™æ˜¯å‘Šè¯‰ setuptoolsï¼šä½ åº”è¯¥å» src/ ç›®å½•ä¸­æŸ¥æ‰¾æ‰€æœ‰æºç /åŒ…ã€‚
- `install_requires=get_requires()` ï¼šå®‰è£…æ—¶æ‰€éœ€è¦çš„ä¾èµ–åŒ…ï¼Œæˆ‘ä»¬å¯ä»¥è¿›å»çœ‹ä¸€ä¸‹ï¼š
    
    ```python
    def get_requires() -> list[str]:
        with open("requirements.txt", encoding="utf-8") as f:
            file_content = f.read()
            lines = [line.strip() for line in file_content.strip().split("\n") if not line.startswith("#")]
            return lines
    ```
    
- `extras_require=extra_require` ï¼šæ„å»ºé¢å¤–ä¾èµ–
- `entry_points={"console_scripts": get_console_scripts()}` ï¼šæˆ‘ä»¬çœ‹ä¸‹ get_console_scripts() å‡½æ•°ï¼š
    
    ```python
    def get_console_scripts() -> list[str]:
        console_scripts = ["llamafactory-cli = llamafactory.cli:main"]
        if os.getenv("ENABLE_SHORT_CONSOLE", "1").lower() in ["true", "y", "1"]:
            console_scripts.append("lmf = llamafactory.cli:main")
    
        return console_scripts
    ```
    
    è¿™å…¶å®æŒ‡å‡ºäº†å‰é¢æˆ‘ä»¬æ‰€å›°æƒ‘çš„`llamafactory-cli` æ˜¯ä¸ªä»€ä¹ˆä¸œè¥¿ï¼Œå®ƒå…¶å®å°±æ˜¯`llamafactory.cli:main` ï¼Œå³è¿è¡Œå‘½ä»¤è¡Œ `llamafactory-cli`ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æŒ‡å‘æ¨¡å— `llamafactory/cli.py` ä¸­çš„ `main()` å‡½æ•°ï¼Œæˆ‘ä»¬ç®€å•çœ‹ä¸€ä¸‹å…¶ä¸­å…³é”®éƒ¨åˆ†ï¼š
    
    ```python
        COMMAND_MAP = {
            "api": run_api,
            "chat": run_chat,
            "env": print_env,
            "eval": run_eval,
            "export": export_model,
            "train": run_exp,
            "webchat": run_web_demo,
            "webui": run_web_ui,
            "version": partial(print, WELCOME),
            "help": partial(print, USAGE),
        }
    
        command = sys.argv.pop(1) if len(sys.argv) > 1 else "help"
        if command == "train" and (is_env_enabled("FORCE_TORCHRUN") or (get_device_count() > 1 and not use_ray())):
            ...
        elif command in COMMAND_MAP:
            COMMAND_MAP[command]()
        else:
            print(f"Unknown command: {command}.\n{USAGE}")
    ```
    
    åˆ°è¿™é‡Œå°±å¾ˆæ˜æ˜¾äº†ï¼Œå½“è¿è¡Œå‘½ä»¤è¡Œ `llamafactory-cli train` å®é™…ä¸Šå°±æ˜¯ç”¨ sys.argv.pop(1) æå–äº† trainï¼Œå¹¶é€šè¿‡ COMMAND_MAP å­—å…¸å°†å…¶æ˜ å°„ä¸º run_expï¼Œå½“ä½¿ç”¨ Ray åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶æ—¶ï¼Œç›´æ¥è¿è¡Œ run_exp() å‡½æ•°ã€‚
    
    COMMAND_MAP å‘Šè¯‰äº†æˆ‘ä»¬ï¼Œä¸åŒè¿è¡Œæ¨¡å¼ä¸‹çš„å¯åŠ¨å…¥å£åœ¨å“ªã€‚
    

`[torch,metrics]` æ˜¯å‰é¢æåˆ°çš„å¯é€‰ç¯å¢ƒä¾èµ–ã€‚

`--no-build-distribution` è¡¨ç¤ºä¸ä½¿ç”¨éš”ç¦»ç¯å¢ƒæ¥æ„å»ºé¡¹ç›®ï¼Œç›®çš„æ˜¯ä¸ºäº†é¿å…é‡å¤ä¸‹è½½ä¾èµ–ã€‚

æœ€åï¼Œè¿™äº›è¢«è§£æçš„å‚æ•°ä¼šæœ€ç»ˆå½¢æˆä¸€ä¸ª wheelï¼Œé€šè¿‡ pip install å®‰è£…æ„å»ºç¯å¢ƒã€‚

# 2. è§£æ train æ¨¡å¼ä¸‹çš„ run_exp()

run_exp() è·¯å¾„ï¼šsrc/llamafactory/train/tuner.py

```python
def run_exp(args: Optional[dict[str, Any]] = None, callbacks: Optional[list["TrainerCallback"]] = None) -> None:
    args = read_args(args)
    if "-h" in args or "--help" in args:
        get_train_args(args)

    ray_args = get_ray_args(args)
    callbacks = callbacks or []
    if ray_args.use_ray: # æ˜¯å¦è°ƒç”¨ Ray åˆ†å¸ƒå¼è®­ç»ƒ
		    # RayTrainReportCallback(), Rayå›è°ƒç±», å°†æ¯ä¸€è½®è®­ç»ƒçš„metricsæ±‡æŠ¥ç»™Ray
        callbacks.append(RayTrainReportCallback()) 
        trainer = get_ray_trainer(
            training_function=_training_function,
            train_loop_config={"args": args, "callbacks": callbacks},
            ray_args=ray_args,
        )
        trainer.fit()
    else:
        _training_function(config={"args": args, "callbacks": callbacks})
```

`read_args()` å‡½æ•°çš„ä½œç”¨æ˜¯è¯»å–é…ç½®å‚æ•°ï¼š

- å¦‚æœä¼ å…¥å­—å…¸ã€åˆ—è¡¨ï¼Œé‚£ä¹ˆç›´æ¥è¿”å›ï¼›
- å¦‚æœä¼ å…¥yamlæ–‡ä»¶ã€jsonæ–‡ä»¶ï¼Œé‚£ä¹ˆè½¬åŒ–ä¸ºå­—å…¸ç±»å‹ï¼š

```python
def read_args(args: Optional[Union[dict[str, Any], list[str]]] = None) -> Union[dict[str, Any], list[str]]:
    r"""Get arguments from the command line or a config file."""
    if args is not None:  # å¦‚æœä¼ å…¥ dict æˆ– listï¼Œç›´æ¥è¿”å›
        return args

    if sys.argv[1].endswith(".yaml") or sys.argv[1].endswith(".yml"):
        override_config = OmegaConf.from_cli(sys.argv[2:]) # OmegaConf.from_cli, ä»å‘½ä»¤è¡Œè§£æå¹¶åŠ è½½
        dict_config = OmegaConf.load(Path(sys.argv[1]).absolute()) # OmegaConf.load, åŠ è½½é…ç½®æ–‡ä»¶
        return OmegaConf.to_container(OmegaConf.merge(dict_config, override_config))  # merge dict_configå’Œoverride_config, å¹¶è½¬åŒ–ä¸ºpythonå­—å…¸
    elif sys.argv[1].endswith(".json"):
        override_config = OmegaConf.from_cli(sys.argv[2:])
        dict_config = OmegaConf.load(Path(sys.argv[1]).absolute())
        return OmegaConf.to_container(OmegaConf.merge(dict_config, override_config))
    else:
        return sys.argv[1:]
```

`get_ray_args()` å‡½æ•°å¦‚ä¸‹ï¼š

```python
def get_ray_args(args: Optional[Union[dict[str, Any], list[str]]] = None) -> RayArguments:
    parser = HfArgumentParser(RayArguments) # è¿”å›ä¸€ä¸ªå°† dict, list, å‘½ä»¤è¡Œç­‰é…ç½®è§£æä¸º RayArguments dataclass çš„è§£æå™¨
    (ray_args,) = _parse_args(parser, args, allow_extra_keys=True) # _parse_args è¿”å›å‚æ•°çš„ dataclassï¼Œå³ä¸€ä¸ª RayArguments ï¼Œè¿”å› tuple å½¢å¼
    return ray_args
```

å®ƒçš„ä½œç”¨æ˜¯è¿”å›ä¸€ä¸ª RayArguments çš„ dataclass ç±»ï¼Œè¿”å›å½¢å¼ä¸º tupleã€‚

- HfArgumentParser() è¿”å›ä¸€ä¸ªè§£æå™¨ parserï¼Œå®ƒå°†å­—å…¸ã€åˆ—è¡¨ã€å‘½ä»¤è¡Œç­‰å‚æ•°é…ç½®è§£æä¸ºä¸€ç§ RayArguments çš„ dataclass ç±»ã€‚
- *_*parser*_*args() è¿”å›å‚æ•°çš„ dataclassï¼Œå³ä¸€ä¸ª RayArguments ï¼Œè¿”å› tuple å½¢å¼ã€‚
    
    ```c
    def _parse_args(
        parser: "HfArgumentParser", args: Optional[Union[dict[str, Any], list[str]]] = None, allow_extra_keys: bool = False
    ) -> tuple[Any]:
        args = read_args(args) # read_args(), è¿”å›å­˜å‚¨å‚æ•°é…ç½®çš„dictæˆ–list
        if isinstance(args, dict): # å°†dictè§£æä¸ºdataclass
            return parser.parse_dict(args, allow_extra_keys=allow_extra_keys)
    		
    		# å°†listè§£æä¸ºdataclassï¼Œè®¾ç½®return_remaining_strings=Trueåï¼Œæ— æ³•è§£æçš„strè¿”å›ä¸æŠ¥é”™
        (*parsed_args, unknown_args) = parser.parse_args_into_dataclasses(args=args, return_remaining_strings=True) 
    
        if unknown_args and not allow_extra_keys:
            print(parser.format_help())
            print(f"Got unknown args, potentially deprecated arguments: {unknown_args}")
            raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {unknown_args}")
    
        return tuple(parsed_args)
    ```
    

`_training_function()` å‡½æ•°å¦‚ä¸‹ï¼š

```python
def _training_function(config: dict[str, Any]) -> None:
    args = config.get("args")
    callbacks: list[Any] = config.get("callbacks")
    # è¿”å›5ä¸ªdataclass, ModelArguments, DataArguments, TrainingArguments, FinetuningArguments, GeneratingArguments
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)

    callbacks.append(LogCallback())
    if finetuning_args.pissa_convert:
        callbacks.append(PissaConvertCallback())

    if finetuning_args.use_swanlab:
        callbacks.append(get_swanlab_callback(finetuning_args))

    if finetuning_args.early_stopping_steps is not None:
        callbacks.append(EarlyStoppingCallback(early_stopping_patience=finetuning_args.early_stopping_steps))

    callbacks.append(ReporterCallback(model_args, data_args, finetuning_args, generating_args))  # add to last

    if finetuning_args.stage == "pt":
        run_pt(model_args, data_args, training_args, finetuning_args, callbacks)
    elif finetuning_args.stage == "sft":
        run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
    elif finetuning_args.stage == "rm":
        run_rm(model_args, data_args, training_args, finetuning_args, callbacks)
    elif finetuning_args.stage == "ppo":
        run_ppo(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
    elif finetuning_args.stage == "dpo":
        run_dpo(model_args, data_args, training_args, finetuning_args, callbacks)
    elif finetuning_args.stage == "kto":
        run_kto(model_args, data_args, training_args, finetuning_args, callbacks)
    else:
        raise ValueError(f"Unknown task: {finetuning_args.stage}.")

    if is_ray_available() and ray.is_initialized():
        return  # if ray is intialized it will destroy the process group on return

    try:
        if dist.is_initialized():
            dist.destroy_process_group()
    except Exception as e:
        logger.warning(f"Failed to destroy process group: {e}.")
```

å¯ä»¥çœ‹å‡ºï¼Œ`_training_function()`  å‡½æ•°é¦–å…ˆå°†ä¼ å…¥çš„å‚æ•°è§£æä¸º5ä¸ªdataclassï¼šModelArguments, DataArguments, TrainingArguments, FinetuningArguments, GeneratingArgumentsï¼Œéšåä¾æ®ä¸åŒçš„ stageï¼ˆ*pt, sft, rm, ppo, dpo, kto*ï¼‰æ‰§è¡Œ run_{}() å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚

å¦‚æœä¸é‡‡ç”¨ Ray åˆ†å¸ƒå¼è®­ç»ƒï¼Œé‚£ä¹ˆç›´æ¥è°ƒç”¨ `_training_function()` å³å¯ï¼Œå¦åˆ™éœ€è¦æ„å»ºä¸€ä¸ªå…³äº Ray çš„ trainerï¼Œè¿™é€šè¿‡ä¸‹é¢å‡½æ•°å®ç°ï¼š

`_get_ray_trainer()` å‡½æ•°å¦‚ä¸‹ï¼š

```python
def get_ray_trainer(
    training_function: Callable,
    train_loop_config: dict[str, Any],
    ray_args: "RayArguments",
) -> "TorchTrainer":
    if not ray_args.use_ray:
        raise ValueError("Ray was not enabled. Please set `USE_RAY=1` to enable ray.")

    if ray_args.ray_init_kwargs is not None:
        ray.init(**ray_args.ray_init_kwargs)

    if ray_args.ray_storage_filesystem is not None:
        # this means we are using s3/gcs
        storage_path = ray_args.ray_storage_path
    else:
        storage_path = Path(ray_args.ray_storage_path).absolute().as_posix()

    trainer = TorchTrainer(
        training_function,
        train_loop_config=train_loop_config,
        scaling_config=ScalingConfig(
            num_workers=ray_args.ray_num_workers,
            resources_per_worker=ray_args.resources_per_worker,
            placement_strategy=ray_args.placement_strategy,
            use_gpu=True,
        ),
        run_config=RunConfig(
            name=ray_args.ray_run_name,
            storage_filesystem=ray_args.ray_storage_filesystem,
            storage_path=storage_path,
        ),
    )
    return trainer
```

è¯¥å‡½æ•°å°†è®­ç»ƒæ‰€éœ€å‡½æ•° _training_function()ï¼Œè®­ç»ƒå‚æ•°é…ç½® training_loop_configï¼Œåˆ†å¸ƒå¼è®­ç»ƒå‚æ•° ray_args å°è£…ä¸ºä¸€ä¸ª TorchTrainer ç±»å‹çš„ trainerã€‚

<aside>
ğŸ’¡

TorchTrainer ç”¨äºåˆ©ç”¨ Ray æ¡†æ¶è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚training_function æŒ‡å®šäº†æ¯ä¸ªåˆ†å¸ƒå¼è®­ç»ƒ worker ä¸Šæ‰§è¡Œçš„ Python ä»£ç ï¼›training_loop_config æŒ‡å®šäº†è®­ç»ƒçš„è¾“å…¥å‚æ•°ï¼›scaling_config æŒ‡å®š Ray åˆ†å¸ƒå¼è®­ç»ƒç›¸å…³å‚æ•°ï¼›run_config æŒ‡å®šè¿è¡Œæ–‡ä»¶åã€å­˜å‚¨è·¯å¾„ç­‰ã€‚

åˆå§‹åŒ–å®Œæˆä¹‹åï¼Œå¯ä»¥ç”¨ trainer.fit() å¯åŠ¨ä¸€ä¸ªåˆ†å¸ƒå¼è®­ç»ƒä½œä¸šã€‚

</aside>

# 3. è§£æ run_sft() å‡½æ•°

run_sft() è·¯å¾„ï¼šsrc/llamafactory/train/sft/workflow.py

```python
def run_sft(
    model_args: "ModelArguments",
    data_args: "DataArguments",
    training_args: "Seq2SeqTrainingArguments",
    finetuning_args: "FinetuningArguments",
    generating_args: "GeneratingArguments",
    callbacks: Optional[list["TrainerCallback"]] = None,
):
    tokenizer_module = load_tokenizer(model_args)
    tokenizer = tokenizer_module["tokenizer"]
    template = get_template_and_fix_tokenizer(tokenizer, data_args)
    dataset_module = get_dataset(template, model_args, data_args, training_args, stage="sft", **tokenizer_module)
    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)
    ...
    
    # Create model card
    create_modelcard_and_push(trainer, model_args, data_args, training_args, finetuning_args)
```

run_sft() å‡½æ•°å·¥ç¨‹æ¯”è¾ƒå¤§ï¼Œå†…éƒ¨å±‚å±‚è°ƒç”¨äº†å…¶ä»–å‡½æ•°ï¼Œä½†æ˜¯çœ‹æ‡‚äº† run_sft() å°±èƒ½å¿«é€Ÿçœ‹æ‡‚å…¶ä»–è®­ç»ƒæ¨¡å¼å¦‚ run_pt()ï¼Œä¸‹é¢ç»™å‡ºæ¯ä¸ªéƒ¨åˆ†çš„é€»è¾‘æ¡†æ¶ã€‚

## 3.1. tokenizer åŠ è½½

```python
**tokenizer_module = load_tokenizer(model_args)**
load_tokenizer # åŠ è½½é¢„è®­ç»ƒçš„tokenierä¸å¯é€‰çš„processor
â”œâ”€â”€ _get_init_kwargs # å°è¯•ä¸‹è½½æ¨¡å‹æƒé‡ptæ–‡ä»¶ï¼Œå¹¶è·å¾—åŠ è½½çš„tokenizerå‚æ•°
â”œâ”€â”€ AutoTokenizer.from_pretrained # åŠ è½½tokenizerï¼Œå¤„ç†æ–‡æœ¬æ¨¡æ€
â”œâ”€â”€ patch_tokenizer # å¯¹åŠ è½½çš„tokenizeré…ç½®è¿›è¡Œåˆé€‚çš„è°ƒæ•´
â”œâ”€â”€ AutoProcessor_from_pretrained # åŠ è½½processorï¼ŒåŒ…å«image processorç­‰ï¼Œå¤„ç†å…¶ä»–æ¨¡æ€
â”œâ”€â”€ patch_processsor # å¯¹åŠ è½½çš„processoré…ç½®è¿›è¡Œåˆé€‚çš„è°ƒæ•´
â””â”€â”€ return: {"tokenizer": tokenizer, "processor": processor}
```

## 3.2. template åŠ è½½

```python
**template = get_template_and_fix_tokenizer(tokenizer, data_args)**
get_template_and_fix_tokenizer
â”œâ”€â”€ register_template # å°†ç°æœ‰å„ç§æ¨¡å‹ä½¿ç”¨çš„templateæ³¨å†Œä¸ºTemplate dataclass
â”œâ”€â”€ template=TEMPLATE[data_args.template] # ä»æ³¨å†Œçš„templateä¸­åŠ è½½æ•°æ®å‚æ•°æŒ‡å®šçš„template
â”œâ”€â”€ data_args.tool_format # if not None, è®¾å®štemplate.format_functionä¸template.format_tools
â”œâ”€â”€ data_args.default_system # if not None, è®¾å®štemplate.default_system
â”œâ”€â”€ data_args.enable_thinking # æ˜¯å¦ä¸ºæ¨ç†æ¨¡å‹å¼€å¯thinking
â”œâ”€â”€ template.fix_special_tokens # ç¡®ä¿ä½¿ç”¨templateå‰ï¼Œtokenizeræœ‰åˆé€‚çš„eos token, pad token, ä»¥åŠè§†ä½œstop wordçš„ç‰¹æ®Štoken
â”œâ”€â”€ template.fix_jinja_template # å°†templateå®šä¹‰çš„Jinjaæ¨¡æ¿å­—ä¸²æ³¨å…¥tokenizerçš„chat_template
â””â”€â”€ return: ***template*** # Template dataclass
```

## 3.3. dataset åŠ è½½

```python
**dataset_module = get_dataset(template, model_args, data_args, training_args, stage="sft", **tokenizer_module)**
get_dataset
â”œâ”€â”€ data_args.tokenized_path # if not None, ä»ç£ç›˜åŠ è½½dataset
â”œâ”€â”€ _get_merged_dataset # åŠ è½½dataset
â”‚		â”œâ”€â”€ _get_dataset_list # è·å¾—è¦åŠ è½½æ•°æ®é›†çš„è·¯å¾„list, list[DatasetAttr]
â”‚   â”œâ”€â”€ _load_single_dataset # åŠ è½½å•ä¸ªæ•°æ®é›†å¹¶å°†æ ·æœ¬ç»„ç»‡æ ¼å¼æ ‡å‡†åŒ–
â”‚   â”‚   â”œâ”€â”€ data_files # éœ€è¦åŠ è½½çš„æ•°æ®é›†çš„è·¯å¾„
â”‚   â”‚   â”œâ”€â”€ load_dataset # ä»è·¯å¾„ä¸­åŠ è½½æ•°æ®é›†
â”‚   â”‚   â”œâ”€â”€ dataset.select # ç¡®ä¿æ•°æ®é›†å¤§å°ä¸dataset_attrå®šä¹‰çš„ä¸€è‡´ï¼Œä¸”ä¸è¶…è¿‡max_samples
â”‚   â”‚   â””â”€â”€ align_dataset # å°†æ•°æ®é›†ä¸­æ ·æœ¬ç»„ç»‡æ ¼å¼æ ‡å‡†åŒ–
â”‚		â”‚		â”‚		â”œâ”€â”€ dataset_converter=get_dataset_converter # ä»DATASET_CONVERTER=[AlpacaDatasetConverter, SharegptDatasetConverter]ä¸­é€‰æ‹©
â”‚   â”‚   â”‚   â””â”€â”€ ***dataset.map(dataset_converter, ...)*** # å°†æ•°æ®é›†æ ·æœ¬æ ¼å¼è½¬åŒ–ä¸º: dict{key: "_prompt", "_response", "_systerm", "tools", "images", "videos", "audios"}
â”‚   â”œâ”€â”€ merge_dataset # ä¾æ®ä¸åŒmix_strategyå°†å¤šä¸ªdatasetåˆæˆä¸ºä¸€ä¸ªdataset  
â”‚   â””â”€â”€ return: Union[Dataset, IterableDataset]
â”œâ”€â”€ _get_process_dataset # å¯¹datasetè¿›è¡Œé¢„å¤„ç†
â”‚   â”œâ”€â”€ dataset_processor=_get_dataset_processor
â”‚   â”‚   â”œâ”€â”€ dataset_processor_class=PackSupervisedDatasetProcessor # é€‰æ‹©processor
â”‚   â”‚   â””â”€â”€ dataset_processor_class(template, tokenizer, processor, data_args) # å®ä¾‹åŒ–
â”‚   â”œâ”€â”€ dataset.map(dataset_processor.preprocess_dataset)
â”‚   â”‚   â”œâ”€â”€ _encode_data_example
â”‚   â”‚   â”‚   â”œâ”€â”€ template.mm_plugin.process_message
â”‚   â”‚   â”‚   â”œâ”€â”€ template.mm_plugin.process_tokenids
â”‚   â”‚   â”‚   â”œâ”€â”€ ***template.encode_multirun***# å°†exampleå¤„ç†ä¸ºé—®ç­”å¯¹çš„token_ids, type: tuple(list[int], list[int])
â”‚   â”‚   â”‚   â””â”€â”€ return: input_ids, ***labels*** # list[int], list[int]
â”‚   â”‚   â”œâ”€â”€ model_inputs # ä½¿ç”¨greedyç­–ç•¥å¯¹è¾“å…¥çš„å¤šä¸ªæ ·æœ¬è¿›è¡ŒpackingèŠ‚çœpaddingç©ºé—´ï¼Œå¹¶è·å–packåsentenceçš„attention_mask
â”‚   â”‚   â””â”€â”€ return model_inputs # dict{key: "input_ids", "attention_mask", "position_ids", "labels", "images", "videos", "audios"}
â”‚   â””â”€â”€ return: Union[Dataset, IterableDataset]
â”œâ”€â”€ split_dataset # åˆ’åˆ†æ•°æ®é›†ä¸ºtrain, val, (test)
â”œâ”€â”€ data_args.tokenized_path # if None, å°†tokenizeåçš„æ•°æ®å­˜åˆ°ç£ç›˜
â”œâ”€â”€ get_dataset_module # å°†åˆ’åˆ†çš„æ•°æ®é›†å­˜åˆ°dictä¸­
â””â”€â”€ return {"train_dataset": train_dataset, "eval_dataset": eval_dataset}
```

## 3.4. model åŠ è½½

```python
**model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)**
load_model
â”œâ”€â”€ _get_init_kwargs
â”œâ”€â”€ load_config # è¯†åˆ«æ¨¡å‹ç±»å‹å¹¶åŠ è½½é…ç½®ï¼Œtype: "PretrainedConfig"
â”œâ”€â”€ patch_config # ä¸ºæ¨¡å‹é…ç½®æ–‡ä»¶è¿›è¡Œé€‚å½“ä¿®æ­£
â”‚   â”œâ”€â”€ configure_attn_implementation
â”‚   â”œâ”€â”€ configure_rope
â”‚   â”œâ”€â”€ configure_longlora
â”‚   â”œâ”€â”€ configure_quantization
â”‚   â”œâ”€â”€ configure_moe
â”‚   â”œâ”€â”€ configure_visual_model
â”‚   â”œâ”€â”€ configure_packing
â”‚   â”œâ”€â”€ configure_kv_cache
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ apply_liger_kernel # liger kernelèƒ½æ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡ï¼Œå¤§å¹…å‡å°‘æ˜¾å­˜ä½¿ç”¨
â”œâ”€â”€ if model_args.use_unsloth: # é€šè¿‡unslothåŠ è½½é¢„è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ if model is None and not lazy_load: # é€šè¿‡HuggingFace.transformersä¸­çš„AutoModelFor{ImageTextToText, CausalLm, ...}åŠ è½½training from scratch/pretrainedæ¨¡å‹
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ if not lazy_load:
â”‚   â”œâ”€â”€ patch_model # è®¾ç½®layernorm,gradient checkpoint,lm_headä¸mm projectorç²¾åº¦ç­‰
â”‚   â”œâ”€â”€ register_autoclass
â”œâ”€â”€ init_adapter # åˆå§‹åŒ–adapter, full/freeze/loraä¸‰ç§æ¨¡å¼ä¸­é€‰æ‹©
â”‚   â”œâ”€â”€ _setup_full_tuning
â”‚   â”œâ”€â”€ _setup_freeze_tuning
â”‚   â”œâ”€â”€ _setup_lora_tuning
â”œâ”€â”€ if add_valuehead:
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ if not is_trainable: # è‹¥æ¨¡å‹ä¸å¤„äºè®­ç»ƒé˜¶æ®µ, å…³é—­æ¢¯åº¦, å°†æ•°æ®ç±»å‹è°ƒæ•´ä¸ºæ¨¡å‹è®¡ç®—ç±»å‹, å¼€å¯model.eval(), å¦åˆ™å¼€å¯model.train()
â”œâ”€â”€ count_parameters # è·å–model trainableå‚æ•°ä¸totalå‚æ•°
â””â”€â”€ return model # type: PretrainedModel
```

## 3.5. è®¾ç½® data_collater

```python
data_collator = SFTDataCollatorWith4DAttentionMask( # æ•°æ®è¿›ä¸€æ­¥å¤„ç†: å¤„ç†å…¶ä»–æ¨¡æ€æ•°æ®è¾“å…¥; 2d mask -> 4D mask
    template=template,
    model=model if not training_args.predict_with_generate else None,
    pad_to_multiple_of=8 if training_args.do_train else None,  # for shift short attention
    label_pad_token_id=IGNORE_INDEX if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id,
    block_diag_attn=model_args.block_diag_attn,
    attn_implementation=getattr(model.config, "_attn_implementation", None),
    compute_dtype=model_args.compute_dtype,
    **tokenizer_module,
)
```

data_collator çš„ä½œç”¨æ˜¯åœ¨åç»­ä¼ å…¥ HuggingFace.transformers.Seq2SeqTrainer åˆå§‹åŒ– trainer æ—¶ï¼Œå¯¹ä¼ å…¥çš„æ•°æ®é›†æ ·æœ¬åšè¿›ä¸€æ­¥æ•´ç†ï¼Œè¿™é‡Œé¦–å…ˆå¯¹é™¤æ–‡æœ¬å¤–å…¶ä»–æ¨¡æ€æ•°æ®é¢„å¤„ç†ï¼Œéšåå°† batch sanmple ä¸­çš„ 2D attention mask è½¬åŒ–ä¸º 4D attention maskã€‚å…¶é€»è¾‘å¦‚ä¸‹ï¼š

```python
def __call__(self, features: list[dict[str, Any]]) -> dict[str, "torch.Tensor"]:
    features = super().__call__(features)
    if self.block_diag_attn and self.attn_implementation != "flash_attention_2":
        features["attention_mask"] = prepare_4d_attention_mask(features["attention_mask"], self.compute_dtype)

    for key, value in features.items():  # cast data dtype for paligemma
        if torch.is_tensor(value) and torch.is_floating_point(value):
            features[key] = value.to(self.compute_dtype)

    return features
```

å¯ä»¥çœ‹å‡ºï¼Œå®ƒé¦–å…ˆè°ƒç”¨çˆ¶ç±»çš„__call__() æ–¹æ³•å¯¹è¾“å…¥çš„ features å¤„ç†ï¼Œå…¶æ ¸å¿ƒæƒ³æ³•ä¸ºï¼šå…ˆè·å–batchä¸­çš„ images, videos ç­‰å…¶ä»–æ¨¡æ€æ•°æ®ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦ä¸ºç©ºï¼Œè‹¥ä¸ºç©ºåˆ™å®šä¹‰ fake_input_ids, fake_images ç­‰ï¼Œéšåé€å…¥ `self.template.mm_plugin.get_mm_inputs` å¾—åˆ°`mm_inputs` ï¼ˆå¯¹äº imageï¼Œæ¶‰åŠå°†å›¾åƒåƒç´ æ•°é‡å‹ç¼©åˆ°æŒ‡å®šèŒƒå›´ç­‰æ“ä½œï¼‰ï¼Œå¹¶ç”¨ mm_inputs æ›´æ–°è¾“å…¥çš„ featuresï¼šfeatures.update(mm_inputs) ã€‚

`prepare_4d_attention_mask` å°† 2D mask è½¬åŒ–ä¸º 4D maskï¼Œå³ `*(batch_size, seq_len) to (batch_size, 1, seq_len, seq_len)`* ã€‚

## 3.6. è®¾ç½® metric

```python
metric_module = {}
if training_args.predict_with_generate: # ç”Ÿæˆå¼é¢„æµ‹è®¡ç®—æŒ‡æ ‡
    metric_module["compute_metrics"] = ComputeSimilarity(tokenizer=tokenizer) # è®¡ç®—æŒ‡æ ‡: rouge-1, rouge-2, rouge-l, bleu-4
elif finetuning_args.compute_accuracy:
    metric_module["compute_metrics"] = ComputeAccuracy() # token-levelé¢„æµ‹å‡†ç¡®ç‡
    metric_module["preprocess_logits_for_metrics"] = eval_logit_processor # è®¡ç®—å…·æœ‰æœ€å¤§å¯èƒ½æ€§çš„tokenä»¥å‡å°‘å†…å­˜å ç”¨

# Keyword arguments for `model.generate`
gen_kwargs = generating_args.to_dict(obey_generation_config=True)
gen_kwargs["eos_token_id"] = [tokenizer.eos_token_id] + tokenizer.additional_special_tokens_ids
gen_kwargs["pad_token_id"] = tokenizer.pad_token_id
```

## 3.7. trainer åˆå§‹åŒ–

```python
trainer = CustomSeq2SeqTrainer(
    model=model,
    args=training_args,
    finetuning_args=finetuning_args,
    data_collator=data_collator,
    callbacks=callbacks,
    gen_kwargs=gen_kwargs,
    **dataset_module,
    **tokenizer_module,
    **metric_module,
)
```

CustomSeq2SeqTrainer ç»§æ‰¿è‡ª HuggingFace.transformers çš„ Seq2SeqTrainerï¼Œè¿™é‡Œå•ç‹¬è®¾ç½®äº†`save_predictions` æ–¹æ³•ï¼Œå¹¶å¯¹ optimizer, schedule ç­‰æ–¹æ³•è¿›è¡Œ overrideã€‚

è¿™ä¸ª class å®ä¾‹åŒ–åï¼Œä¼šå°† dataset_module è§£åŒ…ä¸º train_dataset å’Œ eval_dataset åˆ†åˆ«ç”¨äº train å’Œ eval é˜¶æ®µï¼Œtokenizer_module ä¸ metric_module ä¹Ÿä¼šè§£åŒ…ä¼ å…¥ã€‚åœ¨è®­ç»ƒæ—¶ä¼šè°ƒç”¨ä¼ å…¥çš„ data_collator å¯¹æ•°æ®å¤„ç†ã€‚

## 3.8. training, evaluating, predict

```python
**# Training**
if training_args.do_train:
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
    trainer.save_model() # saveæ¨¡å‹æƒé‡, é…ç½®, tokenizer
    if finetuning_args.include_effective_tokens_per_second:
        train_result.metrics["effective_tokens_per_sec"] = calculate_tps(
            dataset_module["train_dataset"], train_result.metrics, stage="sft"
        )

    trainer.log_metrics("train", train_result.metrics)
    trainer.save_metrics("train", train_result.metrics)
    trainer.save_state() # saveè®­ç»ƒä¸­é—´çŠ¶æ€ï¼ˆä¼˜åŒ–å™¨ç­‰ï¼‰ï¼Œæ–¹ä¾¿æ–­ç‚¹ç»­è®­
    if trainer.is_world_process_zero() and finetuning_args.plot_loss:
        keys = ["loss"]
        if isinstance(dataset_module.get("eval_dataset"), dict):
		        # sum([[a, b], [c, d]], []) â†’ [a, b, c, d]
            keys += sum( 
                [[f"eval_{key}_loss", f"eval_{key}_accuracy"] for key in dataset_module["eval_dataset"].keys()], []
            )
        else:
            keys += ["eval_loss", "eval_accuracy"]

        plot_loss(training_args.output_dir, keys=keys)

if training_args.predict_with_generate:
    tokenizer.padding_side = "left"  # use left-padding in generation

**# Evaluation**
if training_args.do_eval:
    metrics = trainer.evaluate(metric_key_prefix="eval", **gen_kwargs)
    trainer.log_metrics("eval", metrics)
    trainer.save_metrics("eval", metrics)

**# Predict**
if training_args.do_predict:
    logger.warning_rank0_once("Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.")
    predict_results = trainer.predict(dataset_module["eval_dataset"], metric_key_prefix="predict", **gen_kwargs)
    trainer.log_metrics("predict", predict_results.metrics)
    trainer.save_metrics("predict", predict_results.metrics)
    trainer.save_predictions(dataset_module["eval_dataset"], predict_results, generating_args.skip_special_tokens)

# Create model card
create_modelcard_and_push(trainer, model_args, data_args, training_args, finetuning_args)
```

# 4. å®ä¾‹åŒ–æµ‹è¯•

**template = get_template_and_fix_tokenizer(tokenizer, data_args)**

![image.png](images/blogs/llamafactory/image.png)

```python
Template(
format_user=StringFormatter(slots=['<|im_start|>user\n{{content}}<|im_end|>\n<|im_start|>assistant\n'], tool_format=None), 
format_assistant=StringFormatter(slots=['{{content}}<|im_end|>\n'], tool_format=None), 
format_system=StringFormatter(slots=['<|im_start|>system\n{{content}}<|im_end|>\n'], tool_format=None), 
format_function=FunctionFormatter(slots=['{{content}}<|im_end|>\n'], tool_format='qwen'), 
format_observation=StringFormatter(slots=['<|im_start|>user\n<tool_response>\n{{content}}\n</tool_response><|im_end|>\n<|im_start|>assistant\n'], tool_format=None), 
format_tools=ToolFormatter(slots=[], tool_format='qwen'), 
format_prefix=EmptyFormatter(slots=[], tool_format=None), 
default_system='You are a helpful assistant.', 
stop_words=['<|im_end|>'], 
thought_words=('<think>', '</think>'), 
efficient_eos=False, 
replace_eos=True, 
replace_jinja_template=False, 
enable_thinking=True, 
mm_plugin=Qwen2VLPlugin(image_token='<|image_pad|>', video_token='<|video_pad|>', audio_token=None, expand_mm_tokens=True)
)
```

**dataset.map(dataset_converter, ...)**

å¯¹äº 1 ä¸ª sample (SharegptDatasetConverter)

![image.png](image%201.png)

å…ˆæŠ½å‡º â€œmessagesâ€ å¹¶è½¬åŒ–ä¸º aligned_messagesï¼š

![image.png](image%202.png)

```python
[
    {'role': 'user', 'content': '<image>Who are they?'}, 
    {'role': 'assistant', 'content': "They're Kane and Gretzka from Bayern Munich."}, 
    {'role': 'user', 'content': 'What are they doing?<image>'}, 
    {'role': 'assistant', 'content': 'They are celebrating on the soccer field.'}
]
```

éšåé€šè¿‡å¦‚ä¸‹ä»£ç 

```python
else:  # normal example
    prompt = aligned_messages[:-1]
    response = aligned_messages[-1:]

output = {
    "_prompt": prompt,
    "_response": response,
    "_system": system,
    "_tools": example[self.dataset_attr.tools] if self.dataset_attr.tools else "",
    "_images": self._find_medias(example[self.dataset_attr.images]) if self.dataset_attr.images else None,
    "_videos": self._find_medias(example[self.dataset_attr.videos]) if self.dataset_attr.videos else None,
    "_audios": self._find_medias(example[self.dataset_attr.audios]) if self.dataset_attr.audios else None,
}
return output
```

å¾—åˆ° align åçš„æ•°æ®æ ¼å¼ï¼š

![image.png](image%203.png)

```python
{
    '_prompt': [
                   {'role': 'user', 'content': '<image>Who are they?'}, 
 		   {'role': 'assistant', 'content': "They're Kane and Gretzka from Bayern Munich."}, 
		   {'role': 'user', 'content': 'What are they doing?<image>'}
	       ], 
    '_response': [
		     {'role': 'assistant', 'content': 'They are celebrating on the soccer field.'}
		 ], 
    '_system': '', 
    '_tools': '', 
    '_images': ['data\\mllm_demo_data/1.jpg', 'data\\mllm_demo_data/1.jpg'], 
    '_videos': None,
    '_audios': None
}
```

**encoded_pairs = self.template.encode_multirun(self.tokenizer, messages, system, tools)**

åœ¨ encode_multirun ä¹‹å‰ï¼Œä¸€ä¸ª batch çš„ sample ä¼šåšè½¬æ¢ï¼š`list[dict{"str": Any}] -> dict{"str": list[Any]}` ï¼Œéšåä» â€œ_promptâ€ ç­‰ keys ä¸­æå–ç¬¬ i ä¸ª sample çš„å±æ€§ï¼Œä»¥ä¸‹é¢è¿™ä¸ª sample ä¸ºä¾‹ï¼š

```python
{
    "_prompt": [
	           {'content': "Identify the types of technology used in this passage.\nDesign thinking is a human-centered approach to innovation that draws from the designer's toolkit to integrate the needs of people, the possibilities of technology, and the requirements for success.", 'role': 'user'}
	       ],
    "_response": [
		     {'content': 'The technology mentioned in this passage is not specified, but rather is referred to generally as "the possibilities of technology" in the context of the design thinking approach to innovation.', 'role': 'assistant'}
 		 ],
    "_system": '',
    "_tools": '',
    "_images": '',
    "_videos": '',
    "_audios": ''
}
```

éšåå¯¹å•ä¸ª sample ç”¨ä»¥ä¸‹ codeï¼Œé¦–å…ˆå°†é¢„å®šä¹‰çš„ template ä¸­ slot çš„ {{content}} æ›¿æ¢ä¸ºç›¸åº” message ä¸­çš„å†…å®¹ï¼Œä»è€Œå¾—åˆ°æ•°æ®çš„å¯¹è¯æ¨¡æ¿ï¼Œå¹¶å°†å…¶ç”¨ tokenizer ç¼–ç ä¸º list[int]ï¼Œå…¶ä¸­ self ä¸ºå‰é¢åŠ è½½çš„ templateï¼š

```python
for i, message in enumerate(messages):
    elements = []

    if i == 0:
        elements += self.format_prefix.apply() # è¿”å›å®šä¹‰format_prefixæ—¶ä¼ å…¥çš„slot
        if system or tools:
            tool_text = self.format_tools.apply(content=tools)[0] if tools else "" # tool_text: è¿”å›è°ƒç”¨å·¥å…·çš„prompt, self.format_toolsç”±è°ƒç”¨çš„templateè‡ªå®šä¹‰
            elements += self.format_system.apply(content=(system + tool_text)) # å°†å®šä¹‰format_systermçš„slotä¸­çš„{{content}}æ›´æ¢ä¸º(system + tool_text)

    if message["role"] == Role.USER: # format_user, format_assistant, format_observationå·¥ä½œåŸç†åŒformat_systerm
        elements += self.format_user.apply(content=message["content"], idx=str(i // 2)) # ä¸ºä»€ä¹ˆæœ‰ä¸ªidx? A: åŒºåˆ†userå’Œassistant
    elif message["role"] == Role.ASSISTANT:
        elements += self.format_assistant.apply(content=message["content"])
    elif message["role"] == Role.OBSERVATION:
        elements += self.format_observation.apply(content=message["content"])
    elif message["role"] == Role.FUNCTION:
        elements += self.format_function.apply(content=message["content"])
    else:
        raise NotImplementedError("Unexpected role: {}".format(message["role"]))
        
    encoded_messages.append(self._convert_elements_to_ids(tokenizer, elements))
```

```python
# first loop
## elements
[
    '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n', 
    "<|im_start|>user\nIdentify the types of technology used in this passage.\nDesign thinking is a human-centered approach to innovation that draws from the designer's toolkit to integrate the needs of people, the possibilities of technology, and the requirements for success.<|im_end|>\n<|im_start|>assistant\n"
]
## encoded_messages
[
    [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 28301, 1437, 279, 4494, 315, 5440, 1483, 304, 419, 21085, 624, 20470, 7274, 374, 264, 3738, 49382, 5486, 311, 18770, 429, 26643, 504, 279, 14692, 594, 65894, 311, 31072, 279, 3880, 315, 1251, 11, 279, 23607, 315, 5440, 11, 323, 279, 8502, 369, 2393, 13, 151645, 198, 151644, 77091, 198]
]

# second loop
## elements
[
    'The technology mentioned in this passage is not specified, but rather is referred to generally as "the possibilities of technology" in the context of the design thinking approach to innovation.<|im_end|>\n'
]
## encoded_messages
[
    [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 28301, 1437, 279, 4494, 315, 5440, 1483, 304, 419, 21085, 624, 20470, 7274, 374, 264, 3738, 49382, 5486, 311, 18770, 429, 26643, 504, 279, 14692, 594, 65894, 311, 31072, 279, 3880, 315, 1251, 11, 279, 23607, 315, 5440, 11, 323, 279, 8502, 369, 2393, 13, 151645, 198, 151644, 77091, 198], 
    [785, 5440, 9733, 304, 419, 21085, 374, 537, 5189, 11, 714, 4751, 374, 13862, 311, 8789, 438, 330, 1782, 23607, 315, 5440, 1, 304, 279, 2266, 315, 279, 2884, 7274, 5486, 311, 18770, 13, 151645, 198]
]
```

**labels**

```python
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 5440, 9733, 304, 419, 21085, 374, 537, 5189, 11, 714, 4751, 374, 13862, 311, 8789, 438, 330, 1782, 23607, 315, 5440, 1, 304, 279, 2266, 315, 279, 2884, 7274, 5486, 311, 18770, 13, 151645, 198]
```

åš labels æ—¶ï¼Œä¼šå°† prompt éƒ¨åˆ†çš„ token_id æ¢ä¸º -100ï¼Œè¡¨ç¤º input éƒ¨åˆ†ä¸éœ€è¦æ ‡ç­¾ã€‚
