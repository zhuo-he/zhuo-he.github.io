---
title: 'From RL to RLHF'
date: 2025-07-01
permalink: /posts/2025/07/From RL to RLHF/
tags:
  - cool posts
---

This blog is written to record the study of RLHF in LLM, starting from reinforcement learning.

# 1. 强化学习基础

## 1.1. 强化学习概述

强化学习（Reinforcement Learning, RL）研究的是**智能体如何在一个环境中通过采取行动、获得奖励（或惩罚）来学习最优策略，以实现长期目标的最大化。** 

强化学习由两部分组成：智能体（agent）和环境（environment）。在强化学习过程中，智能体与环境一直在交互。智能体在环境中获取某个状态（state）后，它会利用该状态输出一个动作（action），然后这个动作会在环境中被执行，环境会根据智能体采取的动作，输出下一个状态以及当前这个动作带来的奖励（reward）。智能体的目的就是尽可能多地从环境中获取奖励。

![img](/images/blogs/rlhf/1-1.png)

来看一些日常生活中强化学习的例子：（1）股票交易，我们可以不断地买卖股票，然后根据市场给出的反馈来学会怎么去买卖股票才可以让我们的收益最大化。（2）玩雅达利游戏或者其他电脑游戏，我们通过不断试错去探索怎么玩才能通关。

## 1.2. 为什么需要强化学习

关注强化学习非常重要的一个原因是通过强化学习获得的模型可能超越人类表现。在谈论这一点之前先看看经典的监督学习，我们都知道在大规模图像分类的 ImageNet 数据集上进行预训练得到的 ResNet 模型具备非常强大的视觉表征提取能力，它在当时将图像分类的 SOTA 性能更向前推进了一步。但是，ImageNet 数据集中的数据都是由人类标注得到的，这其实为监督学习的性能制定了一个上限，即监督学习算法的上限就是人类的表现，标注结果决定了它的表现永远不可能超越人类。但是对于强化学习，它在环境里面自己探索，有非常大的潜力，它可以获得超越人类的能力的表现，比如 DeepMind 的 AlphaGo 这样一个强化学习的算法可以把人类顶尖的棋手打败。

相较于监督学习，强化学习的训练更加困难，其主要有以下原因：

- 强化学习通常**处理序列数据**，样本不满足独立同分布条件：例如在雅达利游戏中下一帧的动作需要输入上一帧数据决定。
- 环境对于智能体状态的奖励存在延迟，使得**反馈稀疏不即时**，相当于一个**“试错”**的过程；而监督学习有正确的标签，模型可以通过标签修正自己的预测来即时更新模型。

由此可以总结强化学习的一些基本特征：**试错探索**、**延时稀疏反馈**、**序列数据**。

这里对比一下机器学习中的监督学习、无监督学习和强化学习三大基本范式：

| 特性\范式        | 监督学习           | 无监督学习         | 强化学习             |
| ---------------- | ------------------ | ------------------ | -------------------- |
| **监督信号来源** | 人类标注           | 数据内部结构       | 环境反馈             |
| **代表性任务**   | 图像分类、语音识别 | 图像聚类、维度压缩 | 机器人控制、游戏博弈 |
| **数据形式**     | 数据+标签          | 只有数据           | 状态+动作+奖励序列   |
| **学习目标**     | 拟合标签           | 捕捉数据结构       | 最大化累积回报       |
| **反馈时机**     | 每个样本即时反馈   | 无外部反馈         | 环境延时反馈         |

## 1.3. 强化学习基本概念

### 1.3.1. 序列决策

在强化学习中，智能体一直在与环境进行交互：智能体把它的动作输出给环境，环境取得这个动作后会反馈给智能体一个奖励，并把下一步的观测给与智能体，这一过程称为**序列决策过程**。从观测出发，将观测、动作和奖励的序列定义为历史：

$$H_t=o_1,a_1,r_1,...,o_t,a_t,r_t.$$

基于这些历史信息，环境和智能体会分别通过各自的规则来更新各自的**状态**，即环境状态 $$S_t^e=f^e(H_t)$$，智能体状态 $$S_t^a=f^a(H_t)$$。**环境状态**是指某一时刻 t 的完整、真实的内部描述，它包含了决定环境未来动态（马尔可夫性）和当前奖励所需的所有信息。**智能体状态**是指智能体内部维护的、用于决策的、对其所处情境的内部表示。由于智能体往往不可知环境的全局，因此其通过传感器对环境进行感知，在时刻 t 感知到的关于环境的部分、可能有噪声或不完整的信息被称为**观测**。

当智能体观测与环境状态相同时，即智能体能观测到环境的全部信息，那么称这个环境为**完全可观测的**，否则称环境为**部分可观测的**。由于智能体状态的设计目标类似于构建一个关于环境状态的充分统计量，因此在完全可观测下，智能体状态的一个直接选择便是将环境状态设置为自身状态，因此有`环境状态=智能体观测=智能体状态`。下面介绍动作与奖励。

**动作**是智能体在环境中做出的行动，在给定的环境中，有效动作的集合经常被称为动作空间。动作空间被分为离散动作空间（机器人只能前后左右走）和连续动作空间（机器人可以360°任意方向走）。

**奖励**是由环境给的一种标量的反馈信号（scalar feedback signal），这种信号可显示智能体在某一步采取某个策略的表现如何，强化学习的目的就是最大化它的期望长期累积奖励。

### 1.3.2. 智能体的组成与类型

对于一个强化学习智能体，它可能有一个或多个如下的组成成分。

- **策略（policy）**：智能体会用策略来选取下一步的动作。

后续主要讨论完全可观测的环境，因此有 $$o_t=s_t$$，策略就是智能体将输入变为动作的函数。由于延迟奖励，因此输入只有状态 $$s_t$$，输出为动作 $$a_t$$。

策略分为随机性策略与确定性策略。随机项策略输出智能体采取所有可能动作的概率 $$\pi(a\mid s)=p(a_t=a\mid s_t=s)$$，确定性策略输出智能体最优可能采取的动作，即 $$a_t=\text{argmax}_a \pi(a\mid s)$$。

强化学习一般使用随机性策略，因为通过引入随机性可以让智能体更好地探索环境。

- **模型（model）**：模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式。

模型决定了下一步的状态，它由状态转移概率和奖励函数两个部分组成。

对于马尔可夫奖励过程，转移概率仅与当前状态有关，即 $$p(s_{t+1}\mid s_t=s)$$，奖励为 $$R(s_t)=\mathbb{E}_\pi[r_{t+1}\mid s_t=s]$$。对于马尔可夫决策过程，转移概率与当前状态和动作同时相关，转移概率为 $$p(s_{t+1}\mid s_t=s,a_t=a)$$，奖励为 $$R(s_t,a_t)=\mathbb{R}_\pi[r_{t+1}\mid s_t=s,a_t=a]$$。

- **价值函数（value function）**：我们用价值函数来对当前状态进行评估。价值函数用于评估智能体进入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利。

价值评估了智能体在某一时刻 t 的状态 $$s_t$$ 下的期望长期累积奖励，由于我们更关心近期的奖励，因此引入一个**折扣因子** $$\gamma$$，价值函数定义为：

$$V_\pi(s)=\mathbb{E}_\pi[G_t\mid s_t=s]=\mathbb{E}_\pi[\sum_{k=0}^{∞} \gamma^k r_{t+k+1}\mid s_t=s].$$

其中 $$G_t=\sum_{k=0}^{∞} \gamma^k r_{t+k+1}$$ 表示未来某一决策链下的长期累积奖励。 $$V_\pi(s)$$ 被称为状态价值函数，还有一种动作价值函数 $$Q_\pi(s,a)$$，相比 $$V_\pi(s)$$，其未来可以取得的累积奖励期望还取决于动作：

$$Q_\pi(s,a)=\mathbb{E}_\pi[G_t\mid s_t=s,a_t=a]=\mathbb{E}_\pi[\sum_{k=0}^{∞} \gamma^k r_{t+k+1}\mid s_t=s,a_t=a].$$

根据智能体学习的事物不同，可以把智能体进行归类：

- **基于价值的智能体（Value-based agent）**，它显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。->基于价值的迭代
- **基于策略的智能体（Policy-based agent）**，直接学习策略，并没有学习价值函数。->基于策略的迭代
- **演员-评论员智能体（Actor-Critic agent）**，它同时学习策略和价值函数，然后通过两者的交互得到最佳的动作。

to be continued
